\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{color,soul}

\begin{document}

	We would like to thank the respected reviewer for their valuable comments that helped us to improve the quality of this article. We would also like to state that we were unable to add all the requested details directly to the paper due to the page limitation. However, in this reply, we have tried to address all points that are raised by the reviewers to the best of our ability. Also due to small changes in the manuscript, reference numbers have shifted. We have used the reference numbers as discussed by the reviewers and offered the new reference number at the end of each reply.
	
	~
	
	{\bfseries\large Reviewer 1}
	
	\begin{quote}
	Page 3, Line 13, Right Column- Why you have specifically used 5 hidden layers? What will be the impact on performance when using less or more hidden layers and neurons?
	\end{quote}
	
	During our initial \hl{systematic experiments; instead of "studies"}, we have studied the effect of the network architecture on the accuracy. During these studies we have found 5 layers are necessary for ANN to be able estimate the loss curve. Specifically, in SPR dataset, this requirement is evident. Higher number of layers \hl{not only potentially leads to overfitting, but also to an  increase in} the required number of samples to train causing further problems. Similar situation exists for the number of neurons. However, it is possible that there might be a better combination of layers and neurons as we have not performed any exhaustive optimization on the neural network.
	
	A summary of this reply has been added to the paper in Section II-B at the end of the first paragraph.
	
	\begin{quote}
	Page 4, Line 43, Right Column- The paper cited [18] doesn't have 7 inputs as looking from the dataset file (from github) and the mauscript itself. It looks to me that there are 5 inputs. Can you recheck this?
	\end{quote}
	
	We want to thank the reviewer for his keen eyes. Indeed the number of features were wrongly listed in the paper even though there were 5 features listed in that sentence. The mistake is fixed and highlighted in the revised manuscript.
	
	\begin{quote}
	Also, have you used the dataset of [18] along with your own data making it finally as 1117+432 data samples. If yes, how did you merge both datasets as proposed manuscript (d1,d2,d3,etc) data sets has different inputs parameters than [18]. What happens to non-common columns like number of rings input feature as mentioned in [18].
	\end{quote}
	
	Both datasets are treated separately during experiments. Input neurons of the ANN systems are adjusted depending on the dataset used in a particular experiment without altering the rest of the architecture.
	
	
	\begin{quote}
	Figure 4c, Figure 6- Give more details about the paramaters (d1,d2,d3,refrative index,etc) at which this particular actual blue line graph is obtained.
	\end{quote}

	Requested information is added to the respective figures.
	
	
	\begin{quote}
	Page 5, Line 43, Left Column- You have stated that [18]'s model has been failing in Figure 4c. It looks to me that the blue line in Figure 4c is for your proposed PCF structure design, while [18]'s PCF design was a simple hexagonal structure with varying rings. Also, there was no analyte (but you have used refractive index 1.35 as shown in this manuscript) in [18]. So if you compare ML models (your's and [18]) only on the criteria of wavelength as stated in Figure 4c, then [18] model is supposed to fail. This comparison doesn't make much sense as both designs and input features are different. If you disagree, then explain this comparison in more detail and mention the design specification values.
	The similar comparison (your's and [18]) explanation is required for Figure 6.
	\end{quote}
	
	As the respected reviewer pointed out, we have used our SPR dataset in Figure 4c as well as Figure 4a which includes all configurations. The aim here is to show that the changes in the state-of-the-art is necessary to enable the ANN architecture to handle more complex systems. We should point out in here that this improved network is not the focus of our research, it is merely a prerequisite to the more important contribution of GAN phase, which in turn allows us to tackle the more difficult problem. In Figure 6, the comparison is drawn using the proposed architecture with and without GAN phases. The method in [18] is excluded in this graph as the improvement that can obtained by applying GAN phase to the network proposed in [18] does not yield any net benefit. In the text below we would like to explain our rationale for modifying the state-of-the-art to the proposed architecture.
	
	The newly added average MSE values in the Figure 4 shows that the method in [18] is actually better at minimizing the error in these particular cases. Additionally, in the detailed performance analysis table (Table II), it is evident that the method at [18] and proposed method without the GAN phase has comparable results. However, Figure 4c shows that the error rate of both methods are accumulated differently. Even with the limited samples, the method at [18] is a good predictor of the real curve, except the shape of the curve does not actually model the real output; but a very rough estimation of it. The proposed ANN structure predicts the shape of the curve much better, however, it clearly requires more samples to fit that curve to the real output. Therefore, method at [18] cannot be refined further with less-than-ideal generated samples, on the other hand, the proposed architecture can benefit from increased number of samples. This was our aim when designing the proposed architecture. In fact, in our experiments, we have found that GAN phase has nearly no positive effect on the method proposed in [18]. We have excluded these experiments from the paper as it is already over the page limit. One last point we would like to show is that in Table II, in the SPR dataset, the reduction in error achieved by changing the network is less than 10\%, however, when GAN phase is also included, reduction reaches 68\%.
	
	
	\begin{quote}
	Page 5, Line 46, Left Column- "This can be linked to the fact that this method uses full-batch learning and therefore under-fits the model[46]"- There is no surity of this reason if the above comment/query in line 43 is logical. So reframe/reconsider this statement.
	\end{quote}
	
	We have tried both full-batch and mini-batch learning models during our experiments. Mini-batch training generally had an edge over the full-batch training during these experiments. However, we agree with the reviewer that without a comprehensive experiment and its reported results, this claim has no basis. Therefore, we have decided to retract this statement.
	
	\begin{quote}
	Page 7, Line 45, Left Column- "In our experiments, due to optimized dataset the accuracy of the method proposed in Ref. [18] is also improved."- Following from the previous comments- if the comparison is not for the similar designs then you can't state this improvement. So please elaborate on this.
	\end{quote}
	
	We thank the respected reviewer for catching this sentence. We believe the meaning of this sentence has been completely altered during language editing. We have fixed this mix up. 
	
	\begin{quote}
	Page 7, Line 40, Right Column- Recurrent Neural Networks (RNN) generally rely memory to recognise patterns like in speech recognition or text generation and tells what should/will come after. How do you state/propose here than RNN can be used to discover optimal geometrical shapes?- because all the geometrical shapes might be independent of each other.
	\end{quote}
	
	After an internal debate, we have understood the concern of the respected reviewer and decided to drop this claim.
	
	\begin{quote}
	Page 7, Line 43, Right Column- Github link for the code is broken. I hope you will correct it. Looking forward to check your code along with data samples you have used  to obtain results in this manuscript.
	\end{quote}
	
	The given link is our current working repository that includes the paper. We will alter and publish the repository after the acceptance according to the requirements of the journal preprint policies.
	
	~
	
	{\bfseries\large Reviewer 2}
	
	\begin{quote}
	 Please, reread the paper to correct some misspelling words. For example, Abstract (line 3): should it be "researchers"? Another example in "Introduction", page 1, column 2, line 35: "Snell's Total" instead of "Snell?s Total".
	\end{quote}
	
	\hl{We would like to thank the respected reviewer for his sharpness. We have reread the manuscript and corrected spelling errors as well as grammatical errors, as much as we could. }
	
	\begin{quote}
	The authors wrote, "The most important contribution of this research is the use of
the GAN phase, where the available data is expanded to be
used in the training phase". Why use a Generative adversarial network? What would be the advantages and disadvantages of a GAN approach over other possible data generators, such as evolutionary mutation functions? The authors should provide a comparison methodology and discuss differences between GAN and other data generators.
	\end{quote}
	
	\hl{Though the respected reviewer makes a valid point regarding data augmentation techniques, however, our study was not comparative in nature, thus, other data augmentation techniques will be investigated in future work. Furthermore, we have chosen GAN due to its effectiveness and popularity, as we the references we provided demonstrate.}
	
	\begin{quote}
	How did the authors find the ANN architectures? How long did it take to find the ANN designs suitable for the problem being addressed?
	\end{quote}
	
	\hl{By using [18] as a starting point and carrying systematic experimentations, the ANN architecture was structured as the one which stabilized the loss curve and general enough to have acceptable validation errors on both data sets.}
	
	\textcolor{red}{AZ: the second question makes no sense to me. The time to train an ANN is not something that we consider while building the ANN. Queite frankly I don't know how to reply.}
	
	\begin{quote}
	Which kind of performance metric the authors are measuring in Table II? Is it MSE? Please, define it.
	\end{quote}
	
	\hl{We would like to thank the reviewer for noticing the ill defined Table. Yes, the metric is the MSE, as shown in the tilte of Table II in the new manuscript.}
	
	\begin{quote}
	Please, define which is RI (page 4, column 1, line 38).
	\end{quote}
	
	\hl{We very much appreciate the reviewer's remark on the missing full form of RI. RI referes to the Refractive Index as it is indicated in the same line in the new manuscript. }	
	
	
	\begin{quote}
	How do the authors know the ANN is not overfitting in Fig. 3? Have the authors employed any validation during training? Please, provide the ANN models' performance on the validation by showing the validation MSE during training.
	\end{quote}
	
	\hl{During our experiments we used part of the data to validate training. In an attempt to stay within the maximum page limits, we had decided to not include the validation curves.
	 The ANN's performance on the validation is provided in Figure 4 in the new manuscript.}
	
	\begin{quote}
	Please provide the test MSEs for comparison between the proposed method and the ref. [18] (illustrated in Fig. 4).
	\end{quote}
	
	\hl{The requested MSEs are provided on the graphs in the new manuscript.} \\
	\textcolor{red}{AZ: CK Should we discuss the MSEs here? I think you provided the same answer for Reviewer 1 - Q5.}
	
	~
	
	{\bfseries\large Reviewer 3}
	
	\begin{quote}
	I suggest that the authors give more information about the stopping criteria of the GAN. As the author pointed out, human judgment has an important role in the generation of images and audio. So, could the known physical laws about PCF and SPR be used to replace the human role in the GAN system to improve the performance? How does the author verify the validity of the augmented samples?
	\end{quote}

	\hl{We strongly agree with the reviewer's valuable suggestion. In fact the very reason that propelled us to choose WGAN is its stopping criteria. The loss curve of the WGAN converges in a stable manner indicating the end of the training phase, as it measures the distance between the distrubutions of real and generated data, which in return, to a high degree of certainty - probabilistically speaking-, validates our generated data. Due to the number of pages limitation, we decided not to include the loss graph of the WGAN, however, experiments showed that the curve converges in a stable manner to 0.}	
	
	\begin{quote}
	 In GAN and ANN, how many training samples and test samples are used respectively? How are the test samples selected?
	\end{quote}
	
	\hl{For the purpose of saving space we have not included the corresponding table, which includes detailed information about the data sets. 
	For the SPR set,  for the 9 folds, we used 336 samples to train the GAN and the ANN, 48 for validation and 48 for testing. Please note that each geometric configuration has 16 data point per analyte making up to 48 for the 3 analytes used.
	For the PCF set: for the 10 folds, we used about 10$\%$ for testing, about 10$\%$ for validation and the rest for training the ANN and GAN.}	
	
	\begin{quote}
	In Fig 5, the MSE increase when the augmented samples exceed 1000. Why does more samples cause performance degradation?
	\end{quote}
	
	\hl{The importance of real data dilutes as the number of generated data increases, thus, there is a balance to strive for. To support our claim we conducted further experiments and added the correspoding MSEs to the bar chart of Fugure 6.}
	
	\begin{quote}
	In Fig 6, in the case of SPR, the calculated loss spectrum has a large deviation under some specific parameters. Could the authors discuss the reasons for the deviation and how to judge the validity of the calculation result?
	\end{quote}
	
	
\end{document}