@article{kiarashinejad2020knowledge,
  title={Knowledge discovery in nanophotonics using geometric deep learning},
  author={Kiarashinejad, Yashar and Zandehshahvar, Mohammadreza and Abdollahramezani, Sajjad and Hemmatyar, Omid and Pourabolghasem, Reza and Adibi, Ali},
  journal={Advanced Intelligent Systems},
  volume={2},
  number={2},
  pages={1900132},
  year={2020},
  publisher={Wiley Online Library}
}

@article{asano2018optimization,
  title={Optimization of photonic crystal nanocavities based on deep learning},
  author={Asano, Takashi and Noda, Susumu},
  journal={Optics express},
  volume={26},
  number={25},
  pages={32704--32717},
  year={2018},
  publisher={Optical Society of America}
}
@article{chugh2019machine,
  title={Machine learning approach for computing optical properties of a photonic crystal fiber},
  author={Chugh, Sunny and Gulistan, Aamir and Ghosh, Souvik and Rahman, BMA},
  journal={Optics Express},
  volume={27},
  number={25},
  pages={36414--36425},
  year={2019},
  publisher={Optical Society of America}
}
@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
@inproceedings{schlegl2017unsupervised,
  title={Unsupervised anomaly detection with generative adversarial networks to guide marker discovery},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={International conference on information processing in medical imaging},
  pages={146--157},
  year={2017},
  organization={Springer}
}
@inproceedings{zheng2017unlabeled,
  title={Unlabeled samples generated by gan improve the person re-identification baseline in vitro},
  author={Zheng, Zhedong and Zheng, Liang and Yang, Yi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3754--3762},
  year={2017}
}
@inproceedings{frid2018synthetic,
  title={Synthetic data augmentation using GAN for improved liver lesion classification},
  author={Frid-Adar, Maayan and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  booktitle={2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)},
  pages={289--293},
  year={2018},
  organization={IEEE}
}
@article{tanaka2019data,
  title={Data augmentation using GANs},
  author={Tanaka, Fabio Henrique Kiyoiti dos Santos and Aranha, Claus},
  journal={arXiv preprint arXiv:1904.09135},
  year={2019}
}
@article{perez2017effectiveness,
  title={The effectiveness of data augmentation in image classification using deep learning},
  author={Perez, Luis and Wang, Jason},
  journal={arXiv preprint arXiv:1712.04621},
  year={2017}
}
@article{ravuri2019seeing,
  title={Seeing is not necessarily believing: Limitations of biggans for data augmentation},
  author={Ravuri, Suman and Vinyals, Oriol},
  year={2019}
}
@inproceedings{shmelkov2018good,
  title={How good is my GAN?},
  author={Shmelkov, Konstantin and Schmid, Cordelia and Alahari, Karteek},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={213--229},
  year={2018}
}
@article{bhattarai2019sampling,
  title={Sampling Strategies for GAN Synthetic Data},
  author={Bhattarai, Binod and Baek, Seungryul and Bodur, Rumeysa and Kim, Tae-Kyun},
  journal={arXiv preprint arXiv:1909.04689},
  year={2019}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}
@inproceedings{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}
@article{masters2018revisiting,
  title={Revisiting small batch training for deep neural networks},
  author={Masters, Dominic and Luschi, Carlo},
  journal={arXiv preprint arXiv:1804.07612},
  year={2018}
}
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@inproceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1731--1741},
  year={2017}
}
@BOOK{principles,
  TITLE = { Neural Networks and Deep Learning},
  AUTHOR = {Michael Nielsen},
  YEAR = {2019}, 
  LINK = { http://neuralnetworksanddeeplearning.com/},
}
@article{HORNIK1991251,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives."
}
