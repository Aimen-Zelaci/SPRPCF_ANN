\documentclass[draft, a4, 10pt, onecolumn]{IEEEtran}

\usepackage{color,soul}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}

\title{\hl{On the use of Generative adversarial neural networks for computing photonic crystal fiber optical properties}}

\author{Aimen Zelaci, Ahmet Yaşlı, Cem Kalyoncu, and Hüseyin Ademgil}

\maketitle
	
\begin{abstract}
Photonic crystal fibers (PCF) for specific applications are designed and optimized by both industry experts and researches. However, the potential number of combinations possible for a single application is huge. This issue combined by the speed of PCF numerical simulation techniques causes the task to take significant amount of time. As stated in the previous works, artificial neural networks (ANN) can predict the result of numerical simulations much faster. However, there are two issues with the methods proposed previously. Namely, the required number of samples for training and generality of the designed network. In this paper, we have proposed the use of generative adversarial networks (GAN) to augment the real data set to then train a  ANN model.

\end{abstract}

\section{Introduction}

In this paper we propose the use of artificial neural networks (ANN) to compute the loss in photonic crystal fibers in a wide range of configurations. Additionally, in order to minimize the required amount of samples to train the ANN, we employed generative adversarial networks (GAN). As discussed in the experiments section, this setup significantly reduces the error in the prediction compared to the state-of-the-art methods used in the literature.

\hl{Importance of PCF and SPR, written by AY or HA}

Machine learning techniques are coming to the forefront of many fields [ref], surpassing the human performance in many tasks, namely automatic speech recognition, image recognition, natural language processing, drug discovery and toxicology. Additionally, ANN can approximate any function (Universality theorem)[ref]. This fact propelled researchers to widen the applications of ANN even further, including the study of nanophotonic structures [ref], optimization of photonic crystal nanocavities [ref], and more recently, computing optical properties of a photonic crystal fiber [ref].

One of the most difficult challenges that deep learning models face is that they benefit from large amounts of data to train, which may be costly to acquire [ref]. One of the solutions to overcome this issue is to artificially expand the original dataset by the means of generative networks. Introduced by Goodfellow et al., Generative Adversarial Networks (GAN) [ref], proved to be successful in tasks such as synthetic data generation [ref] and synthetically augment the data for deep learning based systems [ref]. 

\hl{Will be moved to methodology} 
hl{GAN power comes from the fact that they discover insights and structures within distributions that allows them to make good approximations to the real data set.} \hl{This is exactly how ANN works too.} In this paper we focus on determining one of the propagation features of a multi-channel Photonic Crystal Fiber (PCF) sensor based on Surface Plasmon Resonance (SPR), that is the confinement loss, by using an Artificial Neural Network model, and a special kind generative networks to expand our limited data set for the purpose of  improving the accuracy of our ANN model. Our aim to eventually surpass the simulations performed using Full-Vectorial Finite Element Method (FV-FEM) to design, optimise and evaluate the sensor performance. A time comparison between the performance of the ANN and classic simulation techniques has been made here [ref]. Furtheremore, a time comparaison is also made in this paper in section $4$.

\hl{Literature survey}

This paper is organized as follows. Section \ref{sec:prop} details the use of GAN to generate additional training samples for ANN as well as the proposed neural network architecture. Photonic crystal fiber design that is used for testing is described in details in section \ref{sec:pcf}. Detailed analysis of the experimental results are discussed in section \ref{sec:exp}. Finally, concluding remarks are made in section \ref{sec:conc}.

\section{Proposed method}
\label{sec:prop}

In this section details of the proposed method is discussed. At the start, a GAN is trained to generate additional data by using training samples. These generated samples are filtered to ensure they fall within the applicable range. Original training samples and remaining samples are joined to train a fully-connected feed-forward multi layer perceptron neural network network. The details of the proposed ANN architecture and the GAN is discussed in the following subsections.


\hl{will be split into the subsections.} We will first train ANN model to predict the confinement loss of a SPR based PCF using the original data set collected. Then we will augment the original training data set with generated samples by the GAN, to improve the performance of the ANN model.
Previous works [ref, ref], demonstrated that random data augmentation actually had weakened the performance of the model. Hence, active researches are ongoing to find ways to optimally sample the generated data [ref], in other words to filter out the "good" generated data after training the GAN. In this paper, we will use a simple if-statement to discard values that fall out of our desired ranges for both the independent variables($n_{analyte}, \lambda, \Lambda, d1, d2, d3$), and the confinement loss. In the following sub-sections we discuss the networks architectures.

\subsection{Artificial neural network design}
\label{ssec:ann}
The ANN model employed in this research is a fully-connected feed-forward MLP (Multi Layer Perceptron) consisting of an input layer, an output layer, and 5 hidden layers. Each hidden layer consists of 50 neurons and uses Rectified Linear Unit (ReLU) activation function. 

Adam [ref] as the optimizer and the mean squared error as the loss/cost function. To reduce overfitting we use the Early stopping method [reference], that is by saving checkpoints where the best validation mean squared error occurred as we iterate. To accelerate training and mitigate the problem of internal covariate shift Batch Normalization algorithm was used [ref].

\subsection{Generative Adversarial network design}
\label{ssec:gan}

GANs consist of two neural networks, as shown in figure (num). The first of which called a discriminator, that gives an estimate of the probability that a given input is real or generated (fake). Whereas the second network is referred to as the generator, which outputs data samples from a random noise vector called a latent variable usually given the symbol $z$ supplied at its input. The error between the discriminator’s output and the actual labels (The real data samples vectors all labeled as 1: the probability of being real) would then be measured by the means of a chosen metric. Introduced by Arjovsky et al [reference] the Wasserstein distance metric (or the earth mover distance) proved to be very effective, instead of discriminating whether an input is real or generated, the discriminator provides a criticism of how far the generated data from the real data is, hence the discriminator network is referred to as the critic in the WGANs. Throughout this paper we will use yet the improved WGAN, the WGAN with Gradient penalty [reference]. The improved WGAN converges in a stable manner with least efforts made to tune the hyper- parameters of both networks constituting the GAN, which can be very daunting.
The WGAN-GP architecture chosen in this work is as follows: both the Critic and the generator networks are a fully-connected feed-forward MLP, having 4 hidden layers, with $ minibatchsize\times2^{number of layers} $ neurons for each hidden layer in the critic network, whereas $ minibatchsize\times2^{2} $ neurons for each hidden layer in the generator network. The input vector for the Critic network consists of the $six$ parameters discussed earlier for the ANN model, in addition the corresponding loss value, giving a total of seven inputs. The generator is supplied with random noise vector having a dimension of $7$, and outputs a vector that resembles that of the input to the critic network. LeakyReLU and ReLU were chosen as the activation functions for the Critic and Generator respectively and Adam as the optimizer for both networks. Finally Batch Normalization technique was applied to the Generator.
\newpage
\section{Photonic crystal fiber design}
\label{sec:pcf}

\hl{PCF design details, written by AY}

\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}

A labeled data-set of only 432 samples was collected in this work, through simulations using FV-FEM. The length of the data-set made the task of building ANN model look quiet impossible, however the results were satisfying to some extent. The data-set consists of the wavelength $\lambda$, index of refraction $n_{analyte} $, air-hole to air-hole distance $ \Lambda $, and the air holes radii per ring $d1$, $d2$ and $d3$, taken as our independent variables. The labels are the confinement loss of the PCF. The set consists of nine different configurations of the geometric properties ($ \Lambda $, d1, d2, d3), for each configuration the confinement loss was calculated for 3 different analytes (Water (n=1.33), Ethanol (n=1.35) and several commercial hydrocarbon mixtures (n=1.36) [35]). Seven configurations were randomly selected for training both the WGAN-GP and the ANN, one configuration was held out for validation, and the last configuration for testing. This data was preprocessed before fed in the networks. The indices of refraction  are very close, which made it quiet difficult for the neural networks to differentiate between them, after many trials, the best choice was to take only the tens digit of ${134, 135, 136}$, giving ${4, 5, 6}$. Finally, we transform the confinement loss to the log scale.

\hl{Metrics used in the comparisons}


\subsection{Performance of ANN}

We train the ANN model for more than 2000 epochs, starting from the original data-set. Then we augment the data by 1000 generated samples from our WGAN-GP, which will be demonstrated in the next section. The following table summarizes the hyper-parameters chosen for the ANN model:\\
\begin{tabular}{||c c c c c||}
\hline
Length of the training data-set & Learning rate&Batch size & $\beta_{1}$ & $\beta_{2}$\\ [0.5ex] 
\hline\hline
336 & $ 1\times10^{-04} $ & 12 & 0.9 & 0.999 \\
\hline
\hline\hline
1000+336 & $ 1\times10^{-04} $ & 12 & 0.9 & 0.999 \\
\hline
\hline\hline
2000 + 336 & $ 2\times10^{-04} $ & 16 & 0.9 & 0.999 \\
\hline
\hline\hline
3000 + 336 & $ 2.5\times10^{-04} $ & 16  & 0.9 & 0.999 \\
\hline
\end{tabular}\\
\newpage
The MSE on the training sets ranged from 0.0030 to 0.0050. For all data sets the MSE on the training sets decreased in an acceptable manner, as shown in the following figure.
\begin{figure}[h]
    \begin{tikzpicture}[scale=.6]
		\begin{axis}[
		xlabel=$Epochs$, 
		ylabel=$Training \space MSE$,
		title={Training MSE of the ANN model},
		grid=both,
		minor grid style={gray!25},
		major grid style={gray!25},
		width=1.1\linewidth,
		no marks]
		\addplot[line width=1pt,solid,color=blue] %
		table{tr_loss_ann.txt};
		\end{axis}
	\end{tikzpicture}
	\caption{}
\end{figure}\\
    \newline
    Next, we plot the predictions the ANN model made after training using only the original data-set.
    \newpage
\begin{figure}[h]
			\begin{tikzpicture}[scale=.6]
			\begin{axis}[
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, 
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 0},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_336.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
			\caption{}
\end{figure}
As you can see, the ANN made somewhat good predictions of the low values of the confinement loss. A close inspection into the data samples collected shows that points are concentrated in the low values interval, i.e the ANN picked up the pattern quiet easily. However, in the higher values interval, the ANN could not see a pattern because points are distant from each other to some extent, even though we transformed the confinement loss to the log scale to actually minimize these distances.

\subsection{Performance boost of GAN}

During training the metric used to monitor the WGAN-GP is the loss function of the Critic network. Its convergence signals the ending of the training phase, shown in figure(num).
In this sub-section we discuss the results of augmenting the real data set with Synthetic data. Figure(num) demonstrates the predictions made by the ANN model after training with different sizes of the Synthetic data set.
This might seem paradoxical to what we mentioned earlier, that ANN model benefit from large amounts of data. There are several reasons that is driving our ANN model predictions in the wrong way. Even though our WGAN-GP seemed well trained, there still a domain gap between the real data and the generated data, or the expansion of data is growing in the wrong direction to that of entirely containing the real data. Furthermore, the generated data might lack realism. Or the sampling strategy adopted is poor[ref, ref, ref]. After all, with 1000 generated data samples the ANN model made quiet accurate predictions with which we are satisfied, that is we have achieved we set out to accomplish at the beginning, to train an ANN model and make accurate prediction on a never-seen-before data.

\begin{figure}[ht]
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			xlabel=$Epochs$, 
			ylabel=$ Training $ $ MSE $,
			title={Training loss function of the Critic network},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[line width=1pt,solid,color=blue]  %
			table{wgan_tr_loss.txt};
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, 
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 1000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_1000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, % \hertz requires SIunits
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 2000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_2000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, % \hertz requires SIunits
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 3000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_3000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\caption{}
\end{figure}
\newpage
It can readily be seen that by augmenting with 1000 samples the ANN model made the best predictions, which then can be better viewed when we plot the confinement loss of the PCF versus the wavelength($\lambda$) in figure(num).
\begin{figure}
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.33}$},
			xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{133.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth, 
			line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{133.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
 \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.34}$},
			xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{134.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth,
			line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{134.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
 \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.35}$},
		    xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{135.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth,line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{135.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\caption{}
\end{figure}
\newpage
\subsection{Computational performance}

\hl{Training and execution time of ANN versus simulation method}


\section{Conclusion}
\label{sec:conc}

\hl{about the improved speed}

\hl{reduced amount of training samples}

\hl{increased generality of the system}

Machine learning approaches has an inherit strength on top of classical simulation methods: they could model hidden parameters in a system which we have no knowledge about. Therefore, not only ANN can improve the speed of PCF simulation, but also accuracy of the simulations. However, this final claim requires further analysis and experimentation.
	
\end{document}
