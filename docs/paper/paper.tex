\documentclass[draft, 10pt]{IEEEtran}

\usepackage{color,soul}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, shapes.arrows, decorations.markings, calc}
\usepackage{pgfplots}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cite}

%for flowcharts
\tikzstyle{testing} = [
	thick,
	->,
	>=stealth
]
\tikzstyle{training} = [
	thick, 
	decoration={
		markings,
		mark=at position 1 with {
			\arrow[semithick]{open triangle 60}
		}
	},
	double distance=1.4pt, 
	shorten >= 5.5pt,
	preaction = {decorate},
	postaction = {
		draw,
		line width=1.4pt,
		white,
		shorten >= 4.5pt
	}
]

\tikzstyle{data} = [
	cylinder, 
	shape border rotate=90, 
	draw,
	minimum width=1.5cm, 
	minimum height=1.2cm,
	text centered, 
	draw=black, 
	line width=0.3mm,
	shape aspect=.1
]

\tikzstyle{operation} = [
	rectangle, 
	rounded corners,
	minimum width=2cm, 
	minimum height=1.2cm,
	text centered, 
	draw=black, 
	line width=0.6mm,
]

\tikzstyle{sample} = [
	rectangle, 
	minimum width=1.2cm, 
	minimum height=1.2cm,
	text centered, 
	draw=black,
	line width=0.3mm,
]

\tikzstyle{metric} = [
	diamond, 
	minimum width=1.2cm, 
	minimum height=1.2cm,
	text centered, 
	draw=black,
	line width=0.3mm,
]

\tikzstyle{filter} = [
	trapezium, 
	trapezium left angle=70, 
	trapezium right angle=110, 
	minimum width=1.5cm, 
	minimum height=1cm, 
	text centered, 
	draw=black, 
	line width=0.5mm
]

\begin{document}

\title{\hl{On the use of Generative adversarial neural networks for computing photonic crystal fiber optical properties}}

\author{Aimen Zelaci, Ahmet Yaşlı, Cem Kalyoncu, and Hüseyin Ademgil}

\maketitle
	
\begin{abstract}
Photonic crystal fibers (PCF) for specific applications are designed and optimized by both industry experts and researches. However, the potential number of combinations possible for a single application is very large. This issue combined by the speed of the commonly used Full Vectorial Finite Element Method (FV-FEM) causes the task to take significant amount of time. As stated in the previous works, artificial neural networks (ANN) can predict the result of numerical simulations much faster. However, there are two issues with the methods proposed previously. Namely, the required number of samples for training and the overall accuracy of these methods. In this paper, we  propose the use of generative adversarial networks (GAN) to augment the real data set to train an ANN model. Experimental analysis suggest that the proposed combination not only accurately predicts the confinement loss even with limited amount of data but also GAN can be used to improve existing methods in the literature. Finally, it is shown that this system can predict the confinement loss over a range of analytes and wavelengths in a completely new set of geometric configuration.
\end{abstract}

\section{Introduction}

\hl{Importance of PCF and SPR, written by AY or HA}
	
Machine Learning (ML) techniques are coming to the forefront of many fields, surpassing human performance in many tasks, namely automatic speech recognition, image recognition, natural language processing, drug discovery and toxicology. Additionally, ANN can approximate any function proven by Universality theorem \cite{HORNIK1991251}. This fact propelled researchers to widen the applications of ANN even further, including the study of nanophotonic structures \cite{kiarashinejad2020knowledge}, optimization of photonic crystal nanocavities \cite{asano2018optimization}, and more recently, computing optical properties of a photonic crystal fiber \cite{chugh2019machine}.

One of the most difficult challenges that deep learning models face is that they benefit from large amounts of data to train, which may be costly to acquire. One of the solutions to overcome this issue is to artificially expand the original training dataset by the means of generative networks. Introduced by Goodfellow et al., Generative Adversarial Networks (GAN) \cite{goodfellow2014generative}, proved to be successful in data generation \cite{schlegl2017unsupervised, zheng2017unlabeled, frid2018synthetic, tanaka2019data, perez2017effectiveness}.

In this paper, we focus on estimating confinement loss, one of the propagation features of multi-channel Photonic Crystal Fiber (PCF) sensors, using artificial neural networks. Specifically, we have based our system on Surface Plasmon Resonance (SPR). However, in the experiments section we have demonstrated that the designed system is generic enough to apply to multiple PCF designs. The most important contribution of this research is the use of GAN phase, where the available data is expanded to be used in the training phase.


\hl{!! Literature survey !!}

\hl{Use of ANN in PCF}

\hl{SPR in PCF}

This paper is organized as follows. Section \ref{sec:prop} details the use of GAN to generate additional training samples for ANN as well as the proposed neural network architecture. Photonic crystal fiber design that is used for testing is described in details in section \ref{sec:pcf}. Detailed analysis of the experimental results are discussed in section \ref{sec:exp}. Finally, concluding remarks are made in section \ref{sec:conc}.

\section{Proposed method}
\label{sec:prop}

In this section details of the proposed method is discussed. Training of the system contains two phases: a GAN phase and regression training phase. At the start of the training, a GAN is trained to generate additional data by using training samples. These generated samples are filtered to ensure they fall within the applicable range. Original training samples and remaining samples are joined to train a fully-connected feed-forward multi layer perceptron neural network. At the end of the training, the ANN that is trained for the regression task will be used to decide the output of a particular input parameters. This architecture is illustrated in Figure \ref{fig:overall}. The details of the proposed ANN architecture and the GAN is discussed in the following subsections.

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=2cm]
\node (collected) [data] {Collected samples};
\node (GAN) [operation,below of=collected,xshift=-2cm] {GAN};
\node (filter) [filter, below of=GAN, text width=2.4cm] {\baselineskip=10pt Filter out-of-range samples\par};
\node (generated) [data, below of=filter] {Generated samples};
\node (ann) [operation,below of=generated,xshift=2cm] {ANN};
\node (testsample) [sample, right of=generated,xshift=2cm,yshift=-1cm, text width=1.2cm] {\baselineskip=10pt Test\par};
\node (output) [sample, below of=testsample,yshift=-0cm, text width=1.2cm] {\baselineskip=10pt Output\par};

\node (legendtrain) [right of=GAN,xshift=2cm] {};
\node (legendtrainend) [below of=legendtrain,yshift=1cm] {Training};
\node (legendtest) [below of=legendtrainend,yshift=1cm] {};
\node (legendtestend) [below of=legendtest,yshift=1cm] {Testing};


\draw [training] (collected) -- (GAN);
\draw [training] (GAN) -- (filter);
\draw [training] (filter) -- (generated);
\draw [training] (generated) -- (ann);
\draw [training] (collected) -- (ann);
\draw [testing] (testsample.west) to [bend right=45] ($(ann.north east)-(0.5,0)$);
\draw [testing] ($(ann.south east)-(0.5,0)$) to [bend right=45] (output.west);

\draw [training] (legendtrain) -- (legendtrainend);
\draw [testing] (legendtest) -- (legendtestend);

	\end{tikzpicture}
	\caption{System architecture}
	\label{fig:overall}
\end{figure}


\subsection{Generative adversarial network design}
\label{ssec:gan}

Generative adversarial networks are introduced in \cite{goodfellow2014generative}. We have employed GAN to augment the number of samples that are used in training. A GAN is composed of two neural networks: generator and discriminator. The aim of the generator is to transform fully randomized data into data that follows the distribution of the original dataset. Discriminator assesses the performance of the generator and provides feedback for training. Instead of directly training generator, it is trained through this feedback. This paradigm avoids over-fitting the data.

It is possible to use different metrics for training in GAN [references here], for this project we have selected WGAN Wasserstein distance metric. This variant has been proposed by Arjovsky et al in \cite{arjovsky2017wasserstein}. In this system, discriminator is named as critic and it measures the distance between the generated data and the real data. The reason behind choosing WGAN over other methods is to be able to determine a stopping criteria. In a regular GAN system, training is stopped when the generated data is deemed viable by an observer. Since GAN is often used in generation of image, video or audio, using a human in the loop is effective. However, in our problem, it is not viable for a human to judge the generated data. Automating this procedure to remove the human in-the-loop can lead to over or under-fitting, which in turn degrades the performance. WGAN uses an adaptive stopping criteria that does not have the issue mentioned above. Additionally, we have selected to incorporate Gradient penalty to improve WGAN proposed in \cite{gulrajani2017improved}. This improved WGAN system converges in a stable manner without having to fine tune hyper-parameters of the system. The flowchart of this system is given in Figure \ref{fig:gan}.

In this work, both the Critic and the generator are fully connected feed forward MLP models. The details of the networks are given in Table \ref{tbl:gandetails}. The output from this system should be input and output pairs that will be used to train ANN part of the system. In our PCF system we have 6 input parameters and a single output parameter making a total of 7 parameters that will be used in the GAN phase. In Section \ref{sec:exp}, we have experimented using a different PCF system with different inputs, resulting a different number of parameters for the system. The generator is supplied with the same number of parameters as its expected output, that is 7 for our PCF system. These parameters are generated using Gaussian noise and is called latent variable. Once the training phase is complete, the generator and the filter is used to augment the number of training samples that are available for ANN.

Previous works \cite{ravuri2019seeing, shmelkov2018good}, demonstrated that it is possible for random data augmentation to weaken the performance of the model. Hence, it is important to sample the generated data in way to prevent performance degradation \cite{bhattarai2019sampling}. In the proposed system, we have included a filtering step for the generated data. This step uses a simple condition to discard the values that fall out of the desired range, for both the independent variables ($n_{analyte}, \lambda, \Lambda, d1, d2, d3$), and the confinement loss.

\begin{figure}
	\centering
	\begin{tikzpicture}[node distance=2cm]
		\node(latent) 	[filter,text width=1.4cm] 		{\baselineskip=10pt Noise generator\par};
		\node(generator)[operation,below of=latent] 	{Generator};
		\node(generated)[data, below of=generator,
						 text width=1.4cm]				{\baselineskip=10pt Generated samples\par};
		\node(collected)[data, right of=generated,
						 xshift=1cm,text width=1.4cm]	{\baselineskip=10pt Collected samples\par};
		\node(critic)	[operation,below of=generated,
						 xshift=1.5cm]					{Critic};
		\node(metric)   [metric,below of=critic,
						 text width=1.4cm, yshift=-.2cm]{\baselineskip=10pt{}Wasserstein distance\par};
		
		\draw [training] (latent) -- (generator) node [pos=.5, left, xshift=-0.1cm] (z) {Latent variable};
		\draw [training] (generator) -- (generated);
		\draw [training] (generated) -| (critic);
		\draw [training] (collected) -| (critic);
		\draw [training] (critic) -- (metric);
		\draw [testing]  (metric.east) - ++(1,0) |-  (critic.east);
		\draw [testing]  (metric.west) - ++(-2.2,0) |-  (generator.west);
	\end{tikzpicture}
	\caption{WGAN training}
	\label{fig:gan}
\end{figure}

\begin{table}
	\centering
	\begin{tabular}{l|l|l}
		\textbf{Parameter} & \textbf{Generator} & \textbf{Critic} \\\hline
		Hidden layers & 5 & 5 \\
		Neurons & $ 2^{2}~batchsize $ & $2^{layers}~batchsize$ \\
		Activation & ReLU & Leaky ReLU \\
		Optimizer & Adam \cite{kingma2014adam} & Adam \cite{kingma2014adam} \\
		Normalization & - & Batch \cite{ioffe2015batch} \\
	\end{tabular}
	\caption{Details of WGAN model}
	\label{tbl:gandetails}
\end{table}


\subsection{Artificial neural network design}
\label{ssec:ann}

The ANN model employed in this research is a fully-connected feed-forward Multi Layer Perceptron (MLP) consists of an input layer, an output layer, and 5 hidden layers. Each hidden layer contains 50 neurons and uses Rectified Linear Unit (ReLU) activation function.  Table \ref{tbl:anndetails} summarizes the details of this model. We have adopted Adam \cite{kingma2014adam} as the optimizer and the mean squared error as the loss/cost function.

Due to the nature of the problem, over-fitting is an important issue. We have used early stopping method to reduce over-fitting. In this method, it is possible to terminate and go back to a previous state if overall validation error increases. Thus instead of being forced into a worse classifier, training is terminated earlier. In addition, we use Batch Normalization algorithm \cite{ioffe2015batch} to accelerate training, reduce the effects of initial randomized state and mitigate the problem of internal covariate shift.

In proposed system, the ANN is trained using the samples from the original dataset and the augmented samples generated by the GAN phase. Please note that the samples that are selected to be used in training is not used in the training phase.

\begin{table}
\centering
    \begin{tabular}{l|l}
		Hidden layers & 6  \\
		\hline
		Neurons per hidden layer & 50  \\
		\hline
		Activation function & Rectified Linear Unit (ReLU)  \\
		\hline
		Optimizer & Adam\cite{kingma2014adam}  \\
		\hline
		Loss function& Mean Squared Error (MSE) \\
    \end{tabular}
	\caption{Detailes of the ANN model}
	\label{tbl:anndetails}
\end{table}


\section{Photonic crystal fiber design}
\label{sec:pcf}

A labelled data set of only 432 samples was collected in this work, through simulations using FV-FEM. This data set consists of the wavelength $\lambda$, index of refraction $n_{analyte} $, air-hole to air-hole distance $ \Lambda $, and the air holes radii per ring $d1$, $d2$ and $d3$, taken as our independent variables. The labels are the confinement loss of the PCF. The set consists of nine different configurations of the geometric properties ($ \Lambda $, d1, d2, d3), for each configuration the confinement loss was calculated for three different analytes (Water (n=1.33), Ethanol (n=1.35) and several commercial hydrocarbon mixtures (n=1.36)).

\hl{We will fix the pattern and geometrical shape of the cladding air holes, and vary the wavelength $\lambda$, index of refraction $n_{analyte} $, air-hole to air-hole distance $ \Lambda $, and the air holes radii per ring $d1$, $d2$ and $d3$. Then the results of this study can be carried to different types of SPR-based PCF sensors, using other suitable deep learning approaches such as Recurrent and Convolutional neural networks. }

\hl{PCF design details, written by AY}

\section{Experiments}
\label{sec:exp}

\subsection{Experimental setup}

In order to prove the effectiveness of the proposed setup, we have performed multiple experiments over two datasets. We have built the first dataset that is used in the experiments. This is \hl{an SPR PCF configuration ... and collected using FV-FEM.} Details of this dataset is explained in Section \ref{sec:pcf}. Seven configurations were randomly selected for training both the GAN and the ANN, one configuration was held out for validation, and the last one is used for testing. Slicing the data this way allowed us to guide the network to predict the loss on a completely different configuration.

Additionally, we have used the dataset that has been used in [paper0]. \hl{This dataset contains 1000 samples. Inputs to this data is .... and outputs are ....} We have only used confinement loss for the output.

\hl{CK: continue here.}


By slicing the data set this way we are guiding the network to learn an entirely new set of geometric properties, and this is the reason what made the task impossible, since we only have nine configurations, as we discussed at the beginning of this subsection. In contrast, randomly shuffling the data set as a whole is an easy task in this situation, and the ANN model would make great predictions on the test set, however a sense of generality towards new geometric sets will be lost.

This data was preprocessed before fed in the networks. The indices of refraction  are very close, which made it quiet difficult for the neural networks to differentiate between them, after many trials, the best choice was to take only the tens digit of ${134, 135, 136}$, giving ${4, 5, 6}$. Finally, we transform the confinement loss to the log scale, for the purpose of shortening the distances between the confinement loss points.

\hl{Metrics used in the comparisons}


\subsection{Performance of ANN}

We train the ANN model for more than 2000 epochs, starting from the original data set. Then we augment the latter set by 1000 generated samples by our WGAN-GP, which will be demonstrated in the next subsection, and train again with the new augmented data set. The following table summarizes the hyper-parameters chosen for the ANN model:
\begin{table}[h]
\centering
\scalebox{1.5}{
    \begin{tabular}{|l|l|l|l|l|}
    \hline
Length of the training data-set & Learning rate& Mini batch size & $\beta_{1}$ & $\beta_{2}$\\
\hline
336 & $ 1\times10^{-04} $ & 8 & 0.9 & 0.999 \\
\hline
336 + 1000 & $ 1\times10^{-04} $ & 8 & 0.9 & 0.999 \\
\hline
336 + 2000 & $ 2\times10^{-04} $ & 16 & 0.9 & 0.999 \\
\hline
336 + 3000 & $ 2.5\times10^{-04} $ & 20  & 0.9 & 0.999 \\
\hline
    \end{tabular}
}
\caption{Hyper-parameters chosen for the ANN model, where $\beta_{1}$ and $\beta_{2}$ are a part of Adam parameters}
\end{table}

\newpage
The MSE on the training sets ranged from 0.0030 to 0.0050. For all training data sets the MSE decreased in an acceptable manner. Next, we plot the predictions the ANN model made on the test set, after training using only the original set. As shown in the following figure.

\begin{figure}[h]
	\hspace*{-0.07\textwidth}
	\begin{subfigure}{.5\textwidth}
    \begin{tikzpicture}
		\begin{axis}[
		xlabel=$Epochs$, 
		ylabel=$Training \space MSE$,
		title={Training MSE of the ANN model},
		grid=both,
		minor grid style={gray!25},
		major grid style={gray!25},
		width=1.1\linewidth,
		no marks]
		\addplot[line width=1pt,solid,color=blue] %
		table{tr_loss_ann.txt};
		\end{axis}
	\end{tikzpicture}
	\end{subfigure}\hspace{0.1\textwidth}
	\begin{subfigure}{.5\textwidth}
	\begin{tikzpicture}
			\begin{axis}[
			legend pos=north west,
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, 
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 0},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_336.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
	\end{subfigure}
	\caption{}
\end{figure}

    \newpage

\subsection{Performance boost of GAN}

During training, the metric used to monitor the WGAN-GP is the loss function of the Critic network. Its convergence signals the ending of the training phase, shown in Figure(4).
In this subsection we discuss the results of augmenting the real data set with generated samples. Figure(4) demonstrates the predictions made by the ANN model after training with different sizes of the data set.
This might look paradoxical to what we mentioned earlier, that ANN model benefit from large amounts of data. There are several reasons for why our ANN model is driven in the wrong direction. Even though our WGAN-GP seemed well trained, there still a domain gap between the real data and the generated data, or the expansion of data is growing in the wrong direction to that of entirely containing the real data. Furthermore, the generated data might lack realism. Or the sampling strategy adopted is poor. But the most convincing reason is the limited original training data set size of 336 samples, used to train the WGAN model in the first place. After all, with 1000 generated data samples the ANN model made quiet accurate predictions with which we are satisfied, that is we have achieved what we set out to accomplish at the beginning, to improve the accuracy of the ANN model by artificially expanding the original set.

\hspace{-0.07\textwidth}
\begin{figure}[h]
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			xlabel=$Epochs$, 
			ylabel=$ Training $ $ MSE $,
			title={Training loss function of the Critic network},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[line width=1pt,solid,color=blue]  %
			table{wgan_tr_loss.txt};
			\end{axis}
			\end{tikzpicture}
\end{subfigure}\hspace{0.05\textwidth}
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			legend pos=north west,
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, 
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 1000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_1000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			legend pos=north west,
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $, 
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 2000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_2000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}\hspace{0.05\textwidth}
\begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			legend pos=north west,
			xlabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			ylabel=$Predicted $ $ loss $ $ in $ $ Log(db/cm) $,
			title={Length of the training dataset = 336 + 3000},
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table{dataset_3000.txt};
			\addlegendentry{$Predicted$}
			\addplot[line width=1pt,solid,color=blue]  %
			table{y.txt};
			\addlegendentry{$Actual$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\caption{}
\end{figure}
\newpage
It can readily be seen that by augmenting with 1000 samples the ANN model made the best predictions, which then can be better viewed when we plot the confinement loss of the PCF versus the wavelength($\lambda$) in Figure(5).
The most important result in these experiments is that the ANN model predicted the correct location of the peak value of the confinement loss distributions for the three different analytes used. 
\begin{figure}
    \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.33}$},
			xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{133.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth, 
			line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{133.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}\hspace{0.05\textwidth}
 \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.34}$},
			xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{134.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth,
			line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{134.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
 \begin{subfigure}{.5\textwidth}
			\begin{tikzpicture}
			\begin{axis}[
			title={$n_{analyte=1.35}$},
		    xlabel=$Wavelength(\lambda) $  $ in $  $ nm $, 
			ylabel=$Actual $ $ loss $ $ in $ $ Log(db/cm) $,
			grid=both,
			minor grid style={gray!25},
			major grid style={gray!25},
			width=1.1\linewidth]
			\addplot[
			only marks,
			mark size=2.9pt, mark=o, color=red, fill=red] %
			table[x={X},y={Y_REAL}]{135.txt};
			\addlegendentry{$Actual$}
			\addplot[smooth,line width=1pt,solid,color=blue]  %
			table[x={X},y={Predictions}]{135.txt};
			\addlegendentry{$Predictions$}
			\end{axis}
			\end{tikzpicture}
\end{subfigure}
\caption{}
\end{figure}
\newpage
\subsection{Computational performance}

\hl{Training and execution time of ANN versus simulation method}


This work had been conduct on a  laptop: 

\begin{itemize}
\item Intel(R) Core(TM) i7-5600 CPU @2.60GHz (4 CPUs)
\item 8GB RAM
\item No GPU on board
\end{itemize}

For comparaison purposes we ran the expirements on a machine with GPU on board:

\hl{CEM PC SPECS}

Epirements conducted on the latter machine will be labeled as GPU based, whereas on the former as CPU based.

By taking advantage of Cloud Computing technology, one does not have to spend large amounts of money to obtain high-specs machine, rather, rent readily available deep learning instances in the cloud for pennies on the dollar. 

The code for this work is available at: \url{https://github.com/Aimen-Zelaci/SPRPCF_ANN/}.
\newline
The elapsed training time of an ANN model depends on the parameters of the model, for example, the number of hidden layers and hidden neurons, mini batch size, data set size, number of epochs, the framework used to code the model, and of course the specifications of the machine.
In this work we fixed the number of epochs to 2000 and executed training using Tensorflow(Keras) and Pytorch frameworks. As demonstrated in Tables(5,6). It is worth mentioning the differences between training with full batch or mini batches. In full batch training, the whole data set is passed to the model for each iteration. In contrast, mini-batch-training, the data set is split into small mini batches of equal sizes, then passed to the model seperately, thus for each epoch there will be exactly length  of the data set devided by the mini batch size steps, and hence slowing down the training process. Previous works \cite{masters2018revisiting, keskar2016large} have firmly proved the advantages of using the correct mini batch size as oppose to large batch sizes. Mini batches allow for more frequent gradient calculations, that results in more stable and reliable training. Perhaps one of the main downsides of using large batch size is the poor generalization resulting in a phenomena know as the "generalization gap". Active research \cite{hoffer2017train} is on going to actually use large batch sizes while mainting the performance of the model and shortning the training runtime. A closely related recent work \cite{chugh2019machine}, conducted their expirements using full batch to train the model. In this work we will use the latter paper availabe code on Github for Pytorch expirements, with slight modifications to fit our purposes. Finally, we compare the performance of the WGAN model, using Tensorflow(Keras) framework, of CPU based to that of GPU based.

\begin{table}[h]
\begin{tabular}{lllll}
    & data set size & mini batch size & Time elapsed (sec) \\
Mini batches    &336 + 0 & 8 & 214 & \\
    &336 + 1000 & 8 & 744 & \\
    &336 + 2000 & 16 & 685  & \\ 
    &336 + 3000 & 20 & 861  & \\
\end{tabular}
\begin{tabular}{lllll}
   & data set size & Time elapsed (sec) \\
Full batch    &336 + 0 &  23 & \\
    &336 + 1000 &  34 & \\
    &336 + 2000 &  45 & \\ 
    &336 + 3000 &  57 & \\
\end{tabular}
\caption{Training performance using Tensorflow(Keras). CPU based training.}
\end{table}
\begin{table}[h]
\begin{tabular}{lllll}
 & data set size & mini batch size & Time elapsed (sec) \\
Mini batches    & 336 + 0 & 8 & 1315 & \\
    & 336 + 1000 & 8 & 6453  & \\
    & 336 + 2000 & 16 & 4751  & \\ 
    & 336 + 3000 & 20 & 5132 & \\
\end{tabular}
\begin{tabular}{lllll}
   &data set size & Time elapsed (sec) \\
   Full batch &336 + 0 &  36  & \\
    & 336 + 1000 &  98  & \\
    & 336 + 2000 &  195  & \\ 
    & 336 + 3000 &  336  & \\
\end{tabular}
\caption{Training performance using Pytorch. CPU based training.}
\end{table}

\hl{WGAN EXCUTION WITH GPU VS CPU BY CEM}

Next, we compare the testing runtime of the ANN model to that of simulation methods used to compute the confinement loss for 3 different analytes and 16 wavelenghts, i.e a total of 48 values of the confinement loss, demonstrated in Table(7). This result is absolutely staggering. The ANN model calculated 48 confinement losses in just 0.17 seconds, while simulations took about 2 hours. Therefore, a well trained ANN model can be used to accurately find the best PCF geometric properties set that maximisez the distance between the confinement loss distributions of each analyte in a matter of seconds to minutes. Hence, accelerate the search for the best PCF sensor.

\begin{table}[h]
\centering
\begin{tabular}{l|l}
      & Time elapsed \\
      \hline
    ANN &  0.17 sec \\
    \hline
    Simulation &  7872 sec = 2.18 hours  \\
\end{tabular}

\caption{ANN vs Simulation test runtime performance}
\end{table}
\newpage

\section{Conclusion}
\label{sec:conc}

\hl{about the improved speed}

\hl{reduced amount of training samples}

\hl{increased generality of the system}

\hl{This Idea is very important}
In this work we built a feed forward ANN model that learned the confinement loss given the geometric properties of a SPR-based PCF sensor with fixed pattern and geometrical shape of the cladding air holes. For different types of sensors, Convolutional and Reccurent neural networks can be of great advantage, to discover the spatial insights of variety of patterns and  geometrical shape of the cladding air holes. The latter will be conducted in future work.

Machine learning approaches has an inherit strength on top of classical simulation methods: they could model hidden parameters in a system which we have no knowledge about. Therefore, not only ANN can improve the speed of PCF simulation, but also accuracy of the simulations. However, this final claim requires further analysis and experimentation.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}	
\end{document}
